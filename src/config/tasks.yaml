classify_question_task:  
  description: >  
    Analyze the provided multi-choice neurology question: {original_question}.  
    Determine its complexity level without answering it:  
    - "simple" - if the question tests straightforward factual recall and does not require integration of multiple concepts or clinical reasoning. 
    - "complex" - if the question requires analyzing multiple factors, layers of clinical reasoning and integration of knowledge.  

  expected_output: >  
    A JSON-structured response containing:  
    {  
      "original_question": "Complete question text with choices",  
      "classification": "simple|complex",  
      "classification_reasoning": "Step-by-step analysis"  
    }  

optimize_question_task:  
  description: >  
    Analyze the provided multi-choice neurology question: {original_question}.  
    Extract key medical concepts, conditions, and anatomical structures.  
    Generate up to 5 optimized query for information retrieval to maximize RAG retrieval effectiveness in order to accurately answer the multi-choice question.

  expected_output: >  
    A JSON-structured response containing:  
    {  
      "original_question": "Complete question text with choices",  
      "extracted_concepts": ["key_term_1", "key_term_2", ...],  
      "optimized_queries": ["query_1", "query_2", ...]
    }  

retrieve_store_knowledge_task:  
  description: >  
    Use the optimized queries provided by the Question Interpreter Agent to retrieve relevant information from the neurology book knowledge base (RAG system).  
    Use the RAG tool to retrieve data for each query and consolidate all retrieved information into a single structured file.

  expected_output: >  
    A JSON-structured response containing:  
    {  
      "original_question": "Complete question text with choices",  
      "retrieval_queries": ["query_1", "query_2", ...],  
    }  

synthesize_answer_task:  
  description: >  
    Use the tool AnswerSynthesisTool with the original neurology exam question and the retrieved knowledge from the RAG to synthesize a comprehensive answer. 
    The tool reads a file contains the information retrieved from RAG, use it as a context and prompt the llm to generate answer.

  expected_output: >  
    A JSON-structured response containing:  
    {  
      "original_question": "Complete question text with choices",  
      "selected_answer": "a|b|c|d",  
      "reasoning": "Explain why this answer is correct and why others are incorrect."  
    }  

validate_answer_task:  
  description: >  
    Validate the answer provided by the Answer Synthesis Agent against the retrieved knowledge and clinical standards.
    The agent will:
    - Read the retrieved knowledge data from the previously stored file
    - Analyze the selected answer and reasoning from the synthesis agent
    - Verify that the answer is consistent with current medical knowledge and supported by evidence
    - Confirm the answer if valid, or suggest corrections with detailed justification if not
    - Provide a final validated answer based on rigorous clinical reasoning standards

  expected_output: >  
    A JSON-structured response containing:  
    {  
      "original_question": "Complete question text with choices",  
      "selected_answer": "The selected answer by previous agent a|b|c|d",  
      "validation_result": "Validation analysis explaining whether the answer is valid and consistent with medical knowledge, citing specific evidence",
      "final_answer": "The validated or corrected answer a|b|c|d"
    }